{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requête HTTP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un requête HTTP est une requête basée sur le protocole TCP, elle fait partie de la couche application de la couche OSI. Elle permet d'accéder aux données mise à disposition sur une adresse IP (ou url résolue par un DNS) et un port. \n",
    "\n",
    "Les deux ports les plus utilisés dans le web sont le 80 pour les sites en HTTP et le 443 pour les sites en HTTPS. HTTPS est une variable du protocole HTTP basé sur le protocole TLS.\n",
    "\n",
    "Il existe de nombreux types de requêtes selon la convention `REST`: \n",
    "- GET\n",
    "- POST\n",
    "- PUT \n",
    "- DELETE\n",
    "- UPDATE.\n",
    "\n",
    "Dans notre cas, nous allons utiliser la plupart du temps des GET et potentiellement des POST. \n",
    "- Le GET permet comme son nom l'indique de récupérer des informations en fonction de certains paramètres. \n",
    "- Le POST nécessite un envoi de données pour récupérer des données. Le body du post est, la plupart du temps, envoyé sous la forme d'un objet JSON.\n",
    "\n",
    "Ces requêtes encapsulent un certain nombre de paramètres qui permettent soient d'identifier une provenance et un utilisateur ou de réaliser différentes actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:47:38.635370Z",
     "start_time": "2024-10-07T19:47:38.599631Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:01.794276Z",
     "start_time": "2024-10-07T19:48:01.690378Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.esiee.fr/\"\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe deux méthodes pour récupérer le contenu de la page :\n",
    "\n",
    "- `response.text` qui permet de retourner le texte sous la forme d'une chaine de charactères.\n",
    "- `response.content` qui permet de récupérer le contenu de la page sous la forme de bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:06.901620Z",
     "start_time": "2024-10-07T19:48:06.898643Z"
    }
   },
   "outputs": [],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:13.168682Z",
     "start_time": "2024-10-07T19:48:13.166007Z"
    }
   },
   "outputs": [],
   "source": [
    "type(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer les 1000 premiers charactères de la page :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:15.251081Z",
     "start_time": "2024-10-07T19:48:15.248157Z"
    }
   },
   "outputs": [],
   "source": [
    "response.text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer les headers HTTP de la réponse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:23.287126Z",
     "start_time": "2024-10-07T19:48:23.284018Z"
    }
   },
   "outputs": [],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut modifier les paramètres de la requête et/ou ses headers. On peut par exemple ajouter un UserAgent (identifiant de l'initiateur de la requête) et un timeout de 10 secondes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:38.532299Z",
     "start_time": "2024-10-07T19:48:38.452433Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers, timeout = 10)\n",
    "response.content[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "\n",
    "- Créer une classe Python permettant de faire des requêtes HTTP.\n",
    "- Cette classe doit utiliser toujours le même UserAgent.\n",
    "- Le TimeOut sera spécifié à chaque appelle avec une valeur par défaut.\n",
    "- Un mécanisme de retry sera mis en place de façon recursive.\n",
    "\n",
    "## Exercice 2\n",
    "\n",
    "- Faire une fonction permettant de supprimer tous les espaces supperflus d'une string\n",
    "- Faire une fonction qui prend une string html et renvois une string intelligible (enlever les caractères spéciaux,\n",
    "- Récupérer le domaine en fonction d'un url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1 \n",
    "# - Créer une classe Python permettant de faire des requêtes HTTP.\n",
    "# - Cette classe doit utiliser toujours le même UserAgent.\n",
    "# - Le TimeOut sera spécifié à chaque appelle avec une valeur par défaut.\n",
    "# - Un mécanisme de retry sera mis en place de façon recursive.\n",
    "\n",
    "class HttpRequester:\n",
    "    def __init__(self, user_agent: str):\n",
    "        self.headers = {'User-Agent': user_agent}\n",
    "\n",
    "    def get(self, url: str, timeout: int = 10, retries: int = 3):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=timeout)\n",
    "            response.raise_for_status()  # Raise an error for bad responses\n",
    "            return response\n",
    "        except requests.RequestException as e:\n",
    "            if retries > 0:\n",
    "                return self.get(url, timeout, retries - 1)\n",
    "            else:\n",
    "                raise e\n",
    "            \n",
    "# Example usage:\n",
    "http_requester = HttpRequester('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36')\n",
    "response = http_requester.get(\"https://www.esiee.fr/\")\n",
    "print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2\n",
    "# - Faire une fonction permettant de supprimer tous les espaces supperflus d'une string\n",
    "# - Faire une fonction qui prend une string html et renvois une string intelligible (enlever les caractères spéciaux,\n",
    "# - Récupérer le domaine en fonction d'un url\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def remove_extra_spaces(text: str) -> str:\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def clean_html_string(html: str) -> str:\n",
    "    text = re.sub(r'<[^>]+>', '', html)  # Remove HTML tags\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)  # Remove HTML entities\n",
    "    return remove_extra_spaces(text)\n",
    "\n",
    "def get_domain_from_url(url: str) -> str:\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Example usage:\n",
    "html_string = \"<html>   <body> Hello &nbsp; World! </body> </html>\"\n",
    "cleaned_string = clean_html_string(html_string)\n",
    "print(cleaned_string)  # Output: \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation du HTML  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, il faut récupérer le code HTML d'un site web à partir d'une requête. Lorsque vous avez récupéré le texte d'un site il faut le parser. Pour cela, on utilise BeautifulSoup qui permet de transformer la structure HTML en objet Python. Cela permet de récupérer efficacement les données qui nous intéresse.\n",
    "\n",
    "Pour les webmasters, le blocage le plus souvent mis en place et un blocage sur le User-Agent. Le User-Agent est un paramètre intégré dans la requête HTTP réalisé par le Navigateur pour envoyer au front des informations basiques :\n",
    "\n",
    "- la version du Navigateur,\n",
    "- la version de l'OS\n",
    "- Le type de gestionnaire graphique (Gecko)\n",
    "- le type de device utilisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de User Agent :  \n",
    "\n",
    "`Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons à utiliser `BeautifulSoup`, il est normalement déjà installé, le cas échéant executez les lignes suivantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:10.823860Z",
     "start_time": "2024-10-07T19:50:10.764935Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour transformer une requête (requests) en objet BeautifulSoup :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:13.979141Z",
     "start_time": "2024-10-07T19:50:13.888552Z"
    }
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour trouver tous les liens d'une page, on récupère la balise `a` qui permet de gérer les liens en HTML :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:19.065788Z",
     "start_time": "2024-10-07T19:50:19.063429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/#content\">Aller au contenu</a>,\n",
       " <a href=\"/#menu\">Aller au menu</a>,\n",
       " <a href=\"/plan-du-site/\">Plan du site</a>,\n",
       " <a href=\"/actualites/journees-portes-ouvertes-2025-2026\" target=\"_blank\" title=\"Ouvre une nouvelle fenêtre\">Journée portes ouvertes le 6 décembre de 13h à 18h. Inscrivez-vous dès maintenant !</a>,\n",
       " <a href=\"/\"><img alt=\"ESIEE PARIS\" class=\"a42-ac-replace-img\" src=\"/typo3conf/ext/esiee_sitepackage/Resources/Public/imgs/svg/logo-esiee.svg\"/></a>,\n",
       " <a href=\"/brochures-1\">Brochures</a>,\n",
       " <a href=\"/informations/etudiantes-et-etudiants\">Espace élèves</a>,\n",
       " <a href=\"/\" hreflang=\"fr-FR\" title=\"Français\">\n",
       " <span>Fr</span>\n",
       " </a>,\n",
       " <a href=\"/en/\" hreflang=\"en-US\" title=\"English\">\n",
       " <span>En</span>\n",
       " </a>,\n",
       " <a href=\"/candidater-1\">Candidater</a>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\")[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi préciser la classe HTML qu'on veut récupérer :\n",
    "\n",
    "```python\n",
    "soup.find_all(class_=\"<CLASS_NAME>\")[0:10]\n",
    "```\n",
    "\n",
    "Ici par exemple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:34.406680Z",
     "start_time": "2024-10-07T19:50:34.400091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<button aria-controls=\"searchbox-header-form\" aria-expanded=\"false\" class=\"toggler\">\n",
       " <i class=\"fa-solid fa-magnifying-glass\"></i>\n",
       " <i class=\"fa-solid fa-xmark\"></i>\n",
       " <span class=\"sr-only\">\n",
       " <span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> la recherche\n",
       " \t\t</span>\n",
       " </button>,\n",
       " <button aria-controls=\"submenu-40\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu : </span>L'école</button>,\n",
       " <button aria-controls=\"submenu-563\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu : </span>Gouvernance et conseils</button>,\n",
       " <button aria-controls=\"submenu-65\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu : </span>Départements d'enseignements et de recherche</button>,\n",
       " <button aria-controls=\"submenu-67\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu : </span>Salles blanches</button>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_=\"toggler\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer le text sans les balises HTML :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:43.103789Z",
     "start_time": "2024-10-07T19:50:43.100752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\nESIEE Paris, l'école de l'innovation technologique | ESIEE Paris\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAller au contenu\\nAller au menu\\nPlan du site\\n\\n\\n\\n\\n\\n\\n\\nJournée portes ouvertes le 6 décembre de 13h à 18h. Inscrivez-vous dès maintenant !\\n\\n\\n\\n\\n\\nMasquer l'alerte\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBrochuresEspace élèves\\n\\n\\n\\nFr\\n\\n\\n\\n\\nEn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAfficherMasquer la recherche\\r\\n\\t\\t\\n\\n\\n\\nSaisissez votre recherche\\xa0:\\n\\nLancer la recherche\\n\\n\\n\\nCandidater\\n\\nAfficherMasquer le menu\\n\\n\\n\\n\\n\\nRetour au menu principalAfficherMasquer le sous menu\\xa0: L'écolePourquoi choisir ESIEE Paris ?AfficherMasquer le sous menu\\xa0: Gouvernance et conseilsGouvernance et conseilsConseil scientifiqueAfficherMasquer le sous menu\\xa0: Départements d'enseignements et de rechercheInformatique et télécommunicationsIngénierie des systèmes cyberphysiquesIngénierie industrielleSanté, énergie et environnement durableManagement, sciences humaines et languesCorps professoralDémarche Qualité et DD&RSAfficherMasquer le sous menu\\xa0: Salles blanchesSalles blanchesÉquipements et procé\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "### Exercice 3\n",
    "\n",
    "Améliorer la classe développé précédemment.\n",
    "\n",
    "- Ajouter une méthode pour récupérer l'objet soup d'un url\n",
    "- Récupérer une liste de User Agent et effectuer une rotation aléatoire sur celui à utiliser\n",
    "- Utiliser cette classe pour parser une page HTML et récupérer : le titre, tous les H1 (si ils existent), les liens vers les images, les liens sortants vers d'autres sites, et le texte principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESIEE Paris, l'école de l'innovation technologique | ESIEE Paris\n"
     ]
    }
   ],
   "source": [
    "# Exercice 3\n",
    "\n",
    "# Améliorer la classe développé précédemment.\n",
    "\n",
    "# - Ajouter une méthode pour récupérer l'objet soup d'un url\n",
    "# - Récupérer une liste de User Agent et effectuer une rotation aléatoire sur celui à utiliser\n",
    "# - Utiliser cette classe pour parser une page HTML et récupérer : le titre, tous les H1 (si ils existent), les liens vers les images, les liens sortants vers d'autres sites, et le texte principal\n",
    "\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class EnhancedHttpRequester(HttpRequester):\n",
    "    USER_AGENTS = [\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "        # Add more user agents as needed\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        user_agent = random.choice(self.USER_AGENTS)\n",
    "        super().__init__(user_agent)\n",
    "\n",
    "    def get_soup(self, url: str, timeout: int = 10, retries: int = 3) -> BeautifulSoup:\n",
    "        response = self.get(url, timeout, retries)\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    def parse_page(self, url: str):\n",
    "        soup = self.get_soup(url)\n",
    "        title = soup.title.string if soup.title else 'No title'\n",
    "        h1_tags = [h1.get_text() for h1 in soup.find_all('h1')]\n",
    "        image_links = [img['src'] for img in soup.find_all('img') if 'src' in img.attrs]\n",
    "        external_links = [a['href'] for a in soup.find_all('a', href=True) if urlparse(a['href']).netloc and urlparse(a['href']).netloc != urlparse(url).netloc]\n",
    "        main_text = clean_html_string(soup.get_text())\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'h1_tags': h1_tags,\n",
    "            'image_links': image_links,\n",
    "            'external_links': external_links,\n",
    "            'main_text': main_text\n",
    "        }\n",
    "    \n",
    "# Example usage:\n",
    "enhanced_requester = EnhancedHttpRequester()\n",
    "parsed_data = enhanced_requester.parse_page(\"https://www.esiee.fr/\")\n",
    "print(parsed_data['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation des appels d'API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Losque le front du site récupère des données sur une API gérée par le back, un appel d'API est réalisé. Cet appel est recensé dans les appels réseaux. Il est alors possible de re-jouer cet appel pour récupérer à nouveau les données. Il est très facile de récupérer ces appels dans l'onglet Network de la console développeur de Chrome ou FireFox. La console vous permet de copier le code CURL de la requête et vous pouvez ensuite la transformer en code Python depuis le site https://curl.trillworks.com/.\n",
    "\n",
    "Souvent les APIs sont bloquées avec certains paramètres. L'API vérifie que dans les headers de la requête HTTP ces paramètres sont présents :\n",
    "* un token généré à la volée avec des protocoles OAuth2 (ou moins développés).\n",
    "* un referer provenant du site web (la source de la requête), très facile à falsifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice \n",
    "### Exercice 4\n",
    "\n",
    "- Utiliser les informations développées plus haut pour récupérer les premiers résultats d'une recherche d'une requête\n",
    "sur Google. \n",
    "\n",
    "Tips : \n",
    "\n",
    "- Ouvrir les outils de développements de Chrome ou Firefox\n",
    "- Onglet Network\n",
    "- Fouiller dans les requêtes pour voir à quoi ressemble un appel API Google\n",
    "- Utilisez beautiful soup pour convertir le contenu de la request en objet et accéder aux balises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice 4\n",
    "#- Utiliser les informations développées plus haut pour récupérer les premiers résultats d'une recherche d'une requête sur Google. \n",
    "\n",
    "# Tips : \n",
    "\n",
    "# - Ouvrir les outils de développements de Chrome ou Firefox\n",
    "# - Onglet Network\n",
    "# - Fouiller dans les requêtes pour voir à quoi ressemble un appel API Google\n",
    "# - Utilisez beautiful soup pour convertir le contenu de la request en objet et accéder aux balises\n",
    "\n",
    "\n",
    "import urllib.parse\n",
    "def google_search(query: str, num_results: int = 10):\n",
    "    search_url = \"https://www.google.com/search\"\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'num': num_results\n",
    "    }\n",
    "    full_url = f\"{search_url}?{urllib.parse.urlencode(params)}\"\n",
    "    \n",
    "    enhanced_requester = EnhancedHttpRequester()\n",
    "    soup = enhanced_requester.get_soup(full_url)\n",
    "    \n",
    "    results = []\n",
    "    for g in soup.find_all('div', class_='g'):\n",
    "        title = g.find('h3')\n",
    "        link = g.find('a', href=True)\n",
    "        if title and link:\n",
    "            results.append({\n",
    "                'title': title.get_text(),\n",
    "                'link': link['href']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "# Example usage:\n",
    "search_results = google_search(\"ESIEE Paris\", num_results=5)\n",
    "for result in search_results:\n",
    "    print(result['title'], result['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice Final  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice Final Utilisez tout ce que vous avez appris pour récupérer des articles de News avec une catégorie. Il est souvent intéressant de partir des flux RSS pour commencer :\n",
    "\n",
    "Les données doivent comprendre :\n",
    "\n",
    "- Le texte important propre\n",
    "- L'url\n",
    "- Le domaine\n",
    "- la catégorie\n",
    "- Le titre de l'article\n",
    "- Le titre de la page\n",
    "- (Facultatif): les imames\n",
    "\n",
    "Tips :\n",
    "- Taper le nom de votre média favoris + RSS (par exemple: https://www.lemonde.fr/rss/)\n",
    "- Aller dans le DOM de la page\n",
    "- Trouver les catégories et les liens vers les articles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ESIEE Paris : deux filières renouvellent leur label SecNumedu par l’ANSSI - https://www.esiee.fr/actualites/esiee-paris-deux-filieres-renouvellent-leur-label-secnumedu-par-lanssi\n",
      "2 Des élèves ESIEE Paris participent au Concours jeunes talents Orange 2025 - https://www.esiee.fr/actualites/des-eleves-esiee-paris-participent-au-concours-jeunes-talents-orange-2025\n",
      "3 Préparez votre rentrée ! - https://www.esiee.fr/actualites/preparez-votre-rentree-2025\n",
      "4 Bienvenue aux admis CPGE 2025 à ESIEE Paris - https://www.esiee.fr/actualites/admissions-cpge-2025\n",
      "5 L'université Gustave Eiffel parmi les 3% des meilleures universités au monde selon CWUR - https://www.esiee.fr/actualites/luniversite-gustave-eiffel-parmi-les-meilleures-universites-au-monde-selon-cwur\n",
      "6 Résultats Parcoursup : félicitations aux nouveaux admis à ESIEE Paris - https://www.esiee.fr/actualites/resultats-parcoursup-felicitations-aux-nouveaux-admis-esiee-paris\n",
      "7 Taxe d'apprentissage 2025 : encouragez l'innovation en choisissant ESIEE Paris ! - https://www.esiee.fr/actualites/taxe-dapprentissage-2025-encouragez-linnovation-en-choisissant-esiee-paris\n",
      "8 ESIEE Paris et l’ENSG-Géomatique lancent un double-diplôme en informatique et géomatique - https://www.esiee.fr/actualites/esiee-paris-et-lensg-geomatique-lancent-un-double-diplome-en-informatique-et-geomatique\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def find_rss_feed(site_url: str, requester: EnhancedHttpRequester) -> str | None:\n",
    "    \"\"\"Try to discover an RSS/Atom feed for the given site_url.\"\"\"\n",
    "    soup = requester.get_soup(site_url)\n",
    "    # look for link rel alternate RSS/Atom\n",
    "    for link in soup.find_all(\"link\", rel=lambda r: r and \"alternate\" in r):\n",
    "        t = (link.get(\"type\") or \"\").lower()\n",
    "        if \"rss\" in t or \"atom\" in t or \"xml\" in t:\n",
    "            href = link.get(\"href\")\n",
    "            if href:\n",
    "                return urljoin(site_url, href)\n",
    "    # try common feed paths\n",
    "    common_paths = [\"/rss\", \"/rss.xml\", \"/feed\", \"/feeds\", \"/atom.xml\"]\n",
    "    for p in common_paths:\n",
    "        candidate = urljoin(site_url, p)\n",
    "        try:\n",
    "            resp = requester.get(candidate, timeout=8, retries=1)\n",
    "            if resp and resp.status_code == 200 and (\"xml\" in (resp.headers.get(\"Content-Type\") or \"\") or resp.text.strip().startswith(\"<\")):\n",
    "                return candidate\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def parse_feed_xml(feed_xml: str) -> list[ET.Element]:\n",
    "    \"\"\"Return list of item/entry elements from feed xml string.\"\"\"\n",
    "    root = ET.fromstring(feed_xml.encode(\"utf-8\"))\n",
    "    items = root.findall(\".//item\")\n",
    "    if not items:\n",
    "        # try atom entries\n",
    "        items = root.findall(\".//{http://www.w3.org/2005/Atom}entry\")\n",
    "        if not items:\n",
    "            # fallback: any <entry> without namespace\n",
    "            items = root.findall(\".//entry\")\n",
    "    return items\n",
    "\n",
    "def extract_text_from_element(el: ET.Element) -> str:\n",
    "    \"\"\"Serialize element to HTML and strip tags using BeautifulSoup for readable text.\"\"\"\n",
    "    raw = ET.tostring(el, encoding=\"utf-8\", method=\"html\")\n",
    "    text = BeautifulSoup(raw, \"html.parser\").get_text(separator=\" \")\n",
    "    return clean_html_string(text)\n",
    "\n",
    "def get_first_text(e: ET.Element, tag_names: list[str]) -> str | None:\n",
    "    for tag in tag_names:\n",
    "        # try with and without common namespaces\n",
    "        node = e.find(tag)\n",
    "        if node is not None and (node.text and node.text.strip()):\n",
    "            return node.text.strip()\n",
    "        # try namespaced variants\n",
    "        for child in e:\n",
    "            if child.tag.lower().endswith(tag.lower()):\n",
    "                if child.text and child.text.strip():\n",
    "                    return child.text.strip()\n",
    "    return None\n",
    "\n",
    "def extract_link_from_item(item: ET.Element) -> str | None:\n",
    "    # 1) <link>text</link>\n",
    "    link = get_first_text(item, [\"link\"])\n",
    "    if link and link.startswith(\"http\"):\n",
    "        return link\n",
    "    # 2) <link href=\"...\"> (Atom)\n",
    "    for child in item.findall(\"link\"):\n",
    "        href = child.get(\"href\")\n",
    "        if href:\n",
    "            return href\n",
    "    # 3) search for enclosure/url\n",
    "    enc = item.find(\"enclosure\")\n",
    "    if enc is not None and enc.get(\"url\"):\n",
    "        return enc.get(\"url\")\n",
    "    return None\n",
    "\n",
    "def scrape_news_from_site(site_url: str, max_articles: int = 10) -> list[dict]:\n",
    "    requester = EnhancedHttpRequester()\n",
    "    feed_url = find_rss_feed(site_url, requester)\n",
    "    if not feed_url:\n",
    "        raise RuntimeError(f\"No RSS/Atom feed found for {site_url}\")\n",
    "    resp = requester.get(feed_url)\n",
    "    items = parse_feed_xml(resp.text)\n",
    "    results = []\n",
    "    for item in items[:max_articles]:\n",
    "        try:\n",
    "            article_title = get_first_text(item, [\"title\"]) or \"\"\n",
    "            article_link = extract_link_from_item(item)\n",
    "            category = get_first_text(item, [\"category\"]) or \"\"\n",
    "            # try to extract description/content from feed item\n",
    "            description = get_first_text(item, [\"description\"]) or \"\"\n",
    "            # sometimes content:encoded is used\n",
    "            if not description:\n",
    "                # search for any child that endswith 'encoded' or 'content'\n",
    "                for child in item:\n",
    "                    if child.tag.lower().endswith(\"encoded\") or child.tag.lower().endswith(\"content\"):\n",
    "                        if child.text:\n",
    "                            description = child.text.strip()\n",
    "                            break\n",
    "            main_text = clean_html_string(description) if description else \"\"\n",
    "            page_title = \"\"\n",
    "            images = []\n",
    "            if article_link:\n",
    "                # fetch article to enrich data\n",
    "                try:\n",
    "                    page_soup = requester.get_soup(article_link, timeout=10, retries=2)\n",
    "                    page_title = page_soup.title.string.strip() if page_soup.title else \"\"\n",
    "                    images = [urljoin(article_link, img.get(\"src\")) for img in page_soup.find_all(\"img\") if img.get(\"src\")]\n",
    "                    if not main_text:\n",
    "                        # fallback to page text\n",
    "                        main_text = clean_html_string(page_soup.get_text())\n",
    "                except Exception:\n",
    "                    pass\n",
    "            domain = get_domain_from_url(article_link or site_url)\n",
    "            results.append({\n",
    "                \"article_title\": article_title,\n",
    "                \"url\": article_link or \"\",\n",
    "                \"domain\": domain,\n",
    "                \"category\": category,\n",
    "                \"page_title\": page_title,\n",
    "                \"text\": remove_extra_spaces(main_text),\n",
    "                \"images\": images\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "# Example usage with provided `url` (and available classes/functions in the notebook):\n",
    "news = scrape_news_from_site(url, max_articles=8)\n",
    "for i, item in enumerate(news, 1):\n",
    "    print(i, item[\"article_title\"], \"-\", item[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataengineertools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
